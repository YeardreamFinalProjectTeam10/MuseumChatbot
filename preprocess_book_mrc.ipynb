{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import Dataset\n",
    "from transformers import BertModel, BertTokenizerFast, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('klue/bert-base')\n",
    "\n",
    "class preprocess_dataset():\n",
    "    def __init__(self, file_name, tokenizer):\n",
    "        self.contexts = []\n",
    "        self.questions = []\n",
    "        self.dataset = self.get_json(file_name)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        for data in self.dataset:\n",
    "            for q_p in data['paragraphs']:\n",
    "                passage = q_p['context']\n",
    "                self.contexts.append(passage)\n",
    "                self.questions.append([])\n",
    "                for q in q_p['qas']:\n",
    "                    if q['is_impossible'] == False:\n",
    "                        self.questions[-1].append(q['question'])\n",
    "\n",
    "        #bm25에 contexts 전달\n",
    "        tokenized_corpus = [self.tokenizer(doc)['input_ids'] for doc in self.contexts]\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    def get_json(self,file_name):\n",
    "        with open(file_name) as f:\n",
    "            return json.load(f)['data']\n",
    "\n",
    "    #using bm25, return top_doc_index\n",
    "    def get_negative_passage_bm25(self, question_row, question_column):\n",
    "        tokenized_question = self.tokenizer(self.questions[question_row][question_column])['input_ids']\n",
    "        top_doc = self.bm25.get_top_n(tokenized_question, self.contexts, n=10)\n",
    "        top_doc_idx = [self.contexts.index(doc) for doc in top_doc]\n",
    "        if question_row in top_doc_idx and len(top_doc_idx) >= 3:\n",
    "            return top_doc_idx[random.randint(3, 1)]\n",
    "        elif len(top_doc_idx) > 3:\n",
    "            return top_doc_idx[1]\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def make_dataset(self, num_data):\n",
    "        question_data = []\n",
    "        pos_passage_data = []\n",
    "        negative_passage_data = []\n",
    "        negative_question_data = []\n",
    "        for row, questions in tqdm(enumerate(self.questions)):\n",
    "            for col, question in tqdm(enumerate(questions)):\n",
    "                top_idx = self.get_negative_passage_bm25(row, col)\n",
    "                if top_idx == -1:\n",
    "                    continue\n",
    "                else:\n",
    "                    if len(self.questions[top_idx]) == 0:\n",
    "                        continue\n",
    "                    if len(self.questions[top_idx]) == 1:\n",
    "                        negative_question_data.append(self.questions[top_idx][0])\n",
    "                    else:\n",
    "                        negative_question_data.append(\n",
    "                            self.questions[top_idx][random.randint(0, len(self.questions[top_idx]) - 1)])\n",
    "                question_data.append(question)\n",
    "                pos_passage_data.append(self.contexts[row])\n",
    "                negative_passage_data.append(self.contexts[top_idx])\n",
    "            if len(question_data)>=num_data:\n",
    "                break\n",
    "\n",
    "        #suffle\n",
    "        index_list = list(range(len(question_data)))\n",
    "        random.shuffle(index_list)\n",
    "\n",
    "        questions = [question_data[idx] for idx in index_list]\n",
    "        pos_passages = [pos_passage_data[idx] for idx in index_list]\n",
    "        negative_passages = [negative_passage_data[idx] for idx in index_list]\n",
    "\n",
    "        return Dataset.from_pandas(pd.DataFrame({'pos_question': questions[:num_data],\n",
    "                                                     'pos_passage': pos_passages[:num_data],\n",
    "                                                     'neg_passage': negative_passages[:num_data]}))\n",
    "\n",
    "#해당 데이터 셋은 AI-HUB의 도서자료 기계독해 데이터셋을 다운받아 넣으면 됩니다.\n",
    "train_dataset_process = preprocess_dataset('book_mrc_train.json', tokenizer)\n",
    "eval_dataset_process = preprocess_dataset('book_mrc_eval.json', tokenizer)\n",
    "\n",
    "train_dataset = train_dataset_process.make_dataset(65000)\n",
    "eval_dataset = eval_dataset_process.make_dataset(8500)\n",
    "\n",
    "train_dataset.save_to_disk('book_mrc_train')\n",
    "eval_dataset.save_to_disk('book_mrc_eval')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
